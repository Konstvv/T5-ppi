{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "data = pd.read_csv('../SENSE-PPI/data/senseppi_data/protein.pairs_9606.tsv', sep='\\t', names=['prot1', 'prot2', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prots = set(data['prot1']).union(set(data['prot2']))\n",
    "prot_degree = {prot:0 for prot in prots}\n",
    "for i, row in data[data['score'] == 1].iterrows():\n",
    "    prot_degree[row['prot1']] += 1\n",
    "    prot_degree[row['prot2']] += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d93585231aa1d27",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prots_from_positive = set(data[data['score'] == 1]['prot1']).union(set(data[data['score'] == 1]['prot2']))\n",
    "prots_from_negative = set(data[data['score'] == 0]['prot1']).union(set(data[data['score'] == 0]['prot2']))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72bdda041d85edd4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('Number of proteins in the positive set:', len(prots_from_positive)\n",
    ", '\\nNumber of proteins in the negative set:', len(prots_from_negative))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd969486d33406b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "positive_degrees = []\n",
    "negative_degrees = []\n",
    "for i, row in data.iterrows():\n",
    "    if row['score'] == 1:\n",
    "        positive_degrees.append(prot_degree[row['prot1']])\n",
    "        positive_degrees.append(prot_degree[row['prot2']])\n",
    "    else:\n",
    "        negative_degrees.append(prot_degree[row['prot1']])\n",
    "        negative_degrees.append(prot_degree[row['prot2']])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f411830d0812adc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.hist(positive_degrees, bins=range(0, 100, 1), alpha=0.9, label='Positive')\n",
    "plt.hist(negative_degrees, bins=range(0, 100, 1), alpha=0.5, label='Negative')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(0, 10000)\n",
    "# plt.xlim(0, 20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2810fccfb332f9b6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52612\n",
      "{'tok1': {'input_ids': tensor([[[30, 16, 17,  6,  7,  7,  6,  8, 11,  8,  7,  5,  0, 13, 10, 17,  0,\n",
      "          16, 19,  3,  0, 19,  2,  2, 15, 14,  8,  9,  1,  7,  7,  8,  8,  5,\n",
      "          19,  0, 17,  4, 14, 19, 11,  7, 19,  8,  3,  6, 15,  0,  5,  7,  4,\n",
      "          11, 15, 19, 10,  8, 19,  9,  3, 12,  3, 17, 11,  7,  6, 10,  0,  0,\n",
      "          13,  8,  6,  4,  9, 11,  7,  0, 12,  9,  8,  3, 15, 11,  0,  3, 10,\n",
      "           2,  0, 14, 11, 13,  5, 16, 11,  6,  6,  6,  9, 14, 15,  1,  1, 17,\n",
      "          10,  4,  1,  9,  1,  1, 13,  2,  4, 13,  5,  9,  6,  2,  5,  3,  1,\n",
      "           9,  5,  9, 11,  6,  8,  8,  7,  9,  9,  5,  8,  8,  4,  0,  4,  8,\n",
      "           5, 14, 14,  9, 12, 13,  6,  0,  9,  6, 11,  3, 13, 11,  9, 11,  5,\n",
      "          11,  4, 11,  5,  5,  3, 13,  9, 11,  6, 15,  0,  6, 19, 10,  1,  1,\n",
      "          16,  7,  3, 16, 17,  0,  6, 11, 14,  0,  6,  4, 11, 16,  9,  8, 10,\n",
      "          13,  7, 14, 12,  8,  9,  3, 16, 15,  5,  5,  5, 10,  0, 11,  9, 15,\n",
      "          15,  7,  0, 16,  9,  8,  1, 15,  8, 15,  7, 18,  6,  6, 13,  2, 10,\n",
      "           0, 10,  0, 11, 15,  6,  3,  6, 10,  7, 19, 11,  6, 16, 11,  8,  4,\n",
      "          17, 12,  6,  9,  2,  0,  7, 13,  1,  2,  5,  0,  5,  4,  9,  4,  2,\n",
      "           4,  9, 11,  1, 13,  4, 10,  1,  2,  2,  8, 10,  4,  7,  4,  4, 19,\n",
      "           0, 10,  4, 10,  4,  7, 10,  3, 12,  4, 10, 17, 10, 12, 14, 10, 17,\n",
      "           8,  4,  4,  7, 16, 13,  2,  7,  4, 13,  4, 10,  4,  2, 13, 17, 12,\n",
      "           4,  4, 14,  1, 13,  6, 10,  6, 11,  4,  4,  3, 13,  2,  4,  4,  1,\n",
      "           7,  4,  4,  2, 27, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])}, 'tok2': {'input_ids': tensor([[[30, 16,  1,  4, 13,  5,  9, 14,  4,  8,  7, 12,  9,  0,  5,  8, 10,\n",
      "           1,  9,  8,  3,  5,  0, 17,  9,  8, 13,  3,  5, 10,  0, 10, 16,  5,\n",
      "           9,  8, 10,  7,  5,  4,  3, 12, 15,  5,  9,  3,  9,  2, 13, 18,  8,\n",
      "          15,  2,  2, 17, 13, 17, 10, 10,  0, 16,  3,  0, 12, 10,  0, 14,  7,\n",
      "          13,  7,  2, 15,  2,  9, 17,  9, 12,  5, 16,  0, 16,  0, 12,  7, 12,\n",
      "           5,  5,  3,  3,  2, 15, 19,  9,  4,  9, 13, 12,  0,  2, 13,  9,  0,\n",
      "           5,  9, 12,  0,  1,  0, 10,  9,  4,  8,  5,  9,  5, 17, 14, 12, 16,\n",
      "           7,  0,  1,  4,  0,  4,  1,  4,  1,  1,  4,  7,  4,  7,  4,  7, 12,\n",
      "           4,  6,  4, 11, 11, 10,  4, 11, 11, 10,  4,  2, 11,  4,  1,  7,  4,\n",
      "           7,  5,  1, 13, 10,  1,  2,  4,  4,  4,  4,  0,  2,  7,  6, 11, 18,\n",
      "           5, 12, 11, 12, 16, 12,  3, 11,  7,  0,  5,  2,  5, 14,  4,  3,  7,\n",
      "          16, 18,  4, 10, 13,  9, 13, 13,  9, 12,  2,  1,  3,  2,  5,  2, 12,\n",
      "           1,  5, 13, 10, 10,  9, 15,  4,  5, 15,  0, 11,  2, 11, 11,  0, 10,\n",
      "          10,  2,  2,  0, 10,  2,  8,  9,  0,  4,  9, 10, 11, 12,  0,  1,  5,\n",
      "          14,  7, 11,  3, 11, 10, 11,  6,  4, 11,  2,  5, 10, 10, 11,  7,  3,\n",
      "          10, 19,  4, 15,  4,  2, 19,  5, 11, 16, 14,  6,  9, 15,  1,  1, 16,\n",
      "           6, 11, 17,  0, 17,  8, 17,  2, 10,  6,  3, 17,  3, 19,  1,  5, 19,\n",
      "           2, 11,  1, 14,  0,  5,  4,  4, 11,  0,  6,  6, 17, 12,  0,  3, 17,\n",
      "           7,  2,  5, 11, 10, 14, 12, 19,  7, 14,  5,  2, 19,  2, 11,  6, 14,\n",
      "           4,  0,  9, 14, 13,  0,  6,  7, 17,  0,  6,  8, 17,  7,  2,  9, 11,\n",
      "          10, 14,  3, 19, 10, 14,  9,  3, 19, 13,  6, 11, 14,  1, 12,  4,  7,\n",
      "          13,  0, 11,  7, 17,  8,  0,  7, 17,  3, 11,  7, 11, 13, 13, 10, 27,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "          29]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])}, 'label': tensor([1])}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 36\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(loader_test):\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28mprint\u001B[39m(batch)\n\u001B[0;32m---> 36\u001B[0m     pred, w1, w2 \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;66;03m# pred = pred[0]\u001B[39;00m\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;66;03m# w1 = w1[0]\u001B[39;00m\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;66;03m# w2 = w2[0]\u001B[39;00m\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28mprint\u001B[39m(pred)\n",
      "File \u001B[0;32m~/PycharmProjects/T5-ppi/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/T5-ppi/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/T5-ppi/model.py:278\u001B[0m, in \u001B[0;36mAttentionModel.forward\u001B[0;34m(self, batch, need_weights)\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, need_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m--> 278\u001B[0m     x1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtok1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    279\u001B[0m     x2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtok2\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m    281\u001B[0m     x1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpositional_encoding(x1)\n",
      "\u001B[0;31mIndexError\u001B[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import PairSequenceData\n",
    "import os\n",
    "from model import AttentionModel\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "max_len = 800\n",
    "\n",
    "dataset_test = PairSequenceData(actions_file=\"../SENSE-PPI/data/dscript_data/human_test.tsv\",\n",
    "                                sequences_file=\"../SENSE-PPI/data/dscript_data/human.fasta\",\n",
    "                                max_len=max_len-2)\n",
    "\n",
    "loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
    "\n",
    "print(len(dataset_test))\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = AttentionModel.add_model_specific_args(parser)\n",
    "# parser = pl.Trainer.add_argparse_args(parser)\n",
    "params, unknown = parser.parse_known_args()\n",
    "\n",
    "params.max_len = max_len\n",
    "params.devices = 1\n",
    "params.accelerator = \"cuda\"\n",
    "\n",
    "model = AttentionModel(params, ntoken=len(dataset_test.tokenizer), embed_dim=256)\n",
    "\n",
    "ckpt = torch.load(\"logs/AttentionModelBase/version_0/checkpoints/chkpt_loss_based_epoch=0-val_loss=0.253-val_BinaryF1Score=0.231.ckpt\")\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "for i, batch in enumerate(loader_test):\n",
    "    print(batch)\n",
    "    pred, w1, w2 = model(batch, need_weights=True)\n",
    "    # pred = pred[0]\n",
    "    # w1 = w1[0]\n",
    "    # w2 = w2[0]\n",
    "    print(pred)\n",
    "    if pred > 0.9:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T15:18:21.639902672Z",
     "start_time": "2024-03-14T15:18:20.677060157Z"
    }
   },
   "id": "f303f98cfbe6b481",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2365172266it [20:23, 1932806.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "with open('../SENSE-PPI/protein.physical.links.full.v12.0.txt', 'r') as f:\n",
    "    with open('string_12.0_700.tsv', 'w') as out:\n",
    "        line = f.readline()\n",
    "        out.write(line)\n",
    "        for line in tqdm(f):\n",
    "            if int(line.strip().split()[-1]) > 700:\n",
    "                out.write(line)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T13:55:59.601278507Z",
     "start_time": "2024-03-15T13:35:35.885148782Z"
    }
   },
   "id": "9a15e4e96a30cca2",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pandas.io.parsers.readers.TextFileReader object at 0x7f28a9615000>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "data_700 = pd.read_csv('string_12.0_700.tsv', sep=' ', chunksize=100000)\n",
    "print(data_700)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T16:59:54.390488601Z",
     "start_time": "2024-03-15T16:59:54.383465137Z"
    }
   },
   "id": "fc8ea267ac10d706",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2257it [16:20,  2.30it/s]\n"
     ]
    }
   ],
   "source": [
    "protein_set = set()\n",
    "for chunk in tqdm(data_700):\n",
    "    prots = set(chunk['protein1']).union(set(chunk['protein2']))\n",
    "    protein_set = protein_set.union(prots)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T15:26:34.460312789Z",
     "start_time": "2024-03-15T15:10:13.764687405Z"
    }
   },
   "id": "55fb51d98fc6faf7",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16504586\n"
     ]
    }
   ],
   "source": [
    "print(len(protein_set))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T17:00:13.097155928Z",
     "start_time": "2024-03-15T17:00:13.093673015Z"
    }
   },
   "id": "2c0cefb6907fa5a9",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59309604it [04:43, 209324.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "with open('protein.sequences.v12.0.fa', 'r') as f:\n",
    "    with open('string_12.0_700.fasta', 'w') as out:\n",
    "        for record in tqdm(SeqIO.parse(f, 'fasta')):\n",
    "            if record.id in protein_set:\n",
    "                out.write('>' + record.id + '\\n' + str(record.seq) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T18:25:58.200073421Z",
     "start_time": "2024-03-15T18:21:14.847979805Z"
    }
   },
   "id": "b483b67c7501500a",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16504586\n"
     ]
    }
   ],
   "source": [
    "with open('string_12.0_700.fasta', 'r') as f:\n",
    "    records = list(SeqIO.parse(f, 'fasta'))\n",
    "    print(len(records))\n",
    "record_ids = set([record.id for record in records])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T18:29:00.412761374Z",
     "start_time": "2024-03-15T18:25:58.202417528Z"
    }
   },
   "id": "639d053c54baa102",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16504586\n",
      "16504586\n"
     ]
    }
   ],
   "source": [
    "print(len(protein_set))\n",
    "print(len(record_ids))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T18:29:00.416224269Z",
     "start_time": "2024-03-15T18:29:00.413893988Z"
    }
   },
   "id": "d4969722fba2b306",
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
