{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601466b-b269-4de0-a4f3-506abfe93b8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --user --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d3338-02f4-4835-9bc3-43cc6e1230b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Memory before loading data: 452.34375 Mb\n",
      "INFO:root:Reading sequences from string12.0_experimental_score_500.fasta\n",
      "INFO:root:Max sequence length set to the length of the largest sequence: 1000\n",
      "INFO:root:Reading sequences from string12.0_experimental_score_500.fasta\n",
      "INFO:root:Max sequence length set to the length of the largest sequence: 1000\n",
      "INFO:root:Memory after loading data: 523.57421875 Mb\n",
      "/linkhome/rech/genjqx01/uku51kl/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                    | Type                  | Params\n",
      "------------------------------------------------------------------\n",
      "0 | valid_metrics           | MetricCollection      | 0     \n",
      "1 | train_metrics           | MetricCollection      | 0     \n",
      "2 | test_metrics            | MetricCollection      | 0     \n",
      "3 | embedding               | Embedding             | 1.0 K \n",
      "4 | positional_encoding     | PositionalEncoding    | 0     \n",
      "5 | transformer_blocks      | Sequential            | 825 K \n",
      "6 | cross_transformer_block | CrossTransformerBlock | 150 K \n",
      "7 | dense_head              | Sequential            | 33    \n",
      "------------------------------------------------------------------\n",
      "976 K     Trainable params\n",
      "0         Non-trainable params\n",
      "976 K     Total params\n",
      "3.905     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genjqx01/uku51kl/.local/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/linkhome/rech/genjqx01/uku51kl/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('val_BinaryMatthewsCorrCoef', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a9f228265a4aee8b4180e99b813a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from dataset import PairSequenceData\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar, EarlyStopping\n",
    "import os\n",
    "import psutil\n",
    "from model import AttentionModel\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_before = process.memory_info().rss\n",
    "\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "\n",
    "# max_len = 802\n",
    "# logging.info('Loading data with max_len + 2 tokens = {}'.format(max_len))\n",
    "logging.info('Memory before loading data: {} Mb'.format(memory_before / 1024 / 1024))\n",
    "\n",
    "dataset = PairSequenceData(actions_file=\"string12.0_experimental_score_500_train.tsv\",\n",
    "                           sequences_file=\"string12.0_experimental_score_500.fasta\",\n",
    "                          chunk_size=1000)\n",
    "\n",
    "dataset_test = PairSequenceData(actions_file=\"string12.0_experimental_score_500_test.tsv\",\n",
    "                                sequences_file=\"string12.0_experimental_score_500.fasta\",\n",
    "                               chunk_size=1000)\n",
    "\n",
    "logging.info('Memory after loading data: {} Mb'.format(process.memory_info().rss / 1024 / 1024))\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = AttentionModel.add_model_specific_args(parser)\n",
    "# parser = pl.Trainer.add_argparse_args(parser)\n",
    "parser.add_argument(\"-f\")\n",
    "params = parser.parse_args()\n",
    "\n",
    "params.max_len = dataset.max_len+2\n",
    "params.batch_size = 2 \n",
    "# params.accelerator = \"gpu\"\n",
    "\n",
    "model = AttentionModel(params, ntoken=len(dataset.tokenizer), embed_dim=32)\n",
    "\n",
    "# ckpt = torch.load(\"logs/AttentionModelBase/version_0/checkpoints/chkpt_loss_based_epoch=13-val_loss=0.085-val_BinaryF1Score=0.851.ckpt\")\n",
    "# model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "# model.load_data(dataset=dataset, valid_size=0.01)\n",
    "train_set = model.train_dataloader(dataset, collate_fn=dataset.collate_fn, shuffle=False)\n",
    "val_set = model.val_dataloader(dataset_test, collate_fn=dataset.collate_fn, shuffle=False)\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(\"logs\", name='AttentionModelBase')\n",
    "\n",
    "callbacks = [\n",
    "    # TQDMProgressBar(refresh_rate=500),\n",
    "    ModelCheckpoint(filename='chkpt_loss_based_{epoch}-{val_loss:.3f}-{val_BinaryF1Score:.3f}', verbose=True,\n",
    "                    monitor='val_loss', mode='min', save_top_k=1),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=10,\n",
    "                  verbose=False, mode=\"min\")\n",
    "]\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", num_nodes=1,\n",
    "                     max_epochs=100,\n",
    "                     logger=logger, callbacks=callbacks)\n",
    "\n",
    "trainer.fit(model, train_set, val_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.10.4",
   "language": "python",
   "name": "module-conda-env-python-3.10.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
